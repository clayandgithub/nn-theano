{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce 820M (CNMeM is enabled with initial size: 95.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import time\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNModel:\n",
    "    def __init__(self, layers, epsilon = 0.01, reg_lambda = 0.01):\n",
    "        self.layers = layers # number of nodes in each layer\n",
    "        self.epsilon = np.float32(epsilon) # learning rate for gradient descent\n",
    "        self.reg_lambda = np.float32(reg_lambda) # regularization strength        \n",
    "        \n",
    "        # Initialize the parameters (W and b) to random values. We need to learn these.        \n",
    "        #np.random.seed(int(time.time()) % 1000)\n",
    "        np.random.seed(0)\n",
    "        hidden_layer_num = len(layers) - 1\n",
    "        if hidden_layer_num != 2:\n",
    "            print 'only support 2 hidden layer'\n",
    "            exit(0)\n",
    "        self.W1 = theano.shared(np.random.randn(layers[0], layers[1]).astype('float32'))\n",
    "        self.b1 = theano.shared(np.zeros(layers[1]).astype('float32'))\n",
    "        self.W2 = theano.shared(np.random.randn(layers[1], layers[2]).astype('float32'))\n",
    "        self.b2 = theano.shared(np.zeros(layers[2]).astype('float32'))\n",
    "    \n",
    "    def train_init(self, train_X, train_y):\n",
    "        num_class = np.max(train_y) + 1\n",
    "        num_examples = len(train_X)\n",
    "        train_y_onehot = np.eye(num_class)[train_y]\n",
    "        \n",
    "        # GPU NOTE: Conversion to float32 to store them on the GPU!\n",
    "        X = theano.shared(train_X.astype('float32'))\n",
    "        y = theano.shared(train_y_onehot.astype('float32'))\n",
    "        \n",
    "        # Forward propagation\n",
    "        z1 = X.dot(self.W1) + self.b1\n",
    "        a1 = T.tanh(z1)\n",
    "        z2 = a1.dot(self.W2) + self.b2\n",
    "        y_hat = T.nnet.softmax(z2)\n",
    "        \n",
    "        #Prediction\n",
    "        prediction = T.argmax(y_hat, axis=1)\n",
    "\n",
    "        #Loss function\n",
    "        loss_reg = 1. / num_examples * self.reg_lambda / 2 * (T.sum(T.sqr(self.W1)) + T.sum(T.sqr(self.W2)))\n",
    "        loss = T.nnet.categorical_crossentropy(y_hat, y).mean() + loss_reg\n",
    "\n",
    "        # Gradients\n",
    "        dW2 = T.grad(loss, self.W2)\n",
    "        db2 = T.grad(loss, self.b2)\n",
    "        dW1 = T.grad(loss, self.W1)\n",
    "        db1 = T.grad(loss, self.b1)\n",
    "\n",
    "        # Note that we removed the input values because we will always use the same shared variable\n",
    "        # GPU NOTE: Removed the input values to avoid copying data to the GPU.\n",
    "        forward_prop = theano.function([], y_hat)\n",
    "        predict_by_train_X = theano.function([], prediction)\n",
    "        self.calculate_loss = theano.function([], loss)\n",
    "        self.gradient_step = theano.function(\n",
    "            [],\n",
    "            #profile=True,\n",
    "            updates=((self.W2, self.W2 - self.epsilon * dW2),\n",
    "                     (self.W1, self.W1 - self.epsilon * dW1),\n",
    "                     (self.b2, self.b2 - self.epsilon * db2),\n",
    "                     (self.b1, self.b1 - self.epsilon * db1)))\n",
    "\n",
    "    # This function learns parameters for the neural network from training dataset\n",
    "    # - num_passes: Number of passes through the training data for gradient descent\n",
    "    # - print_loss: If True, print the loss every 1000 iterations\n",
    "    def train(self, train_X, train_y, num_passes=20000, print_loss=False):\n",
    "        self.train_init(train_X, train_y)\n",
    "        \n",
    "        # Gradient descent. For each batch...\n",
    "        for i in xrange(0, num_passes):\n",
    "            # This will update our parameters Ws and bs!\n",
    "            self.gradient_step()\n",
    "\n",
    "            # Optionally print the loss.\n",
    "            # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "            if print_loss and i % 1000 == 0:\n",
    "                print \"Loss after iteration %i: %f\" %(i, self.calculate_loss())\n",
    "\n",
    "        self.predict_init()\n",
    "\n",
    "    def predict_init(self):\n",
    "        X = T.fmatrix('X')\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = X.dot(self.W1) + self.b1\n",
    "        a1 = T.tanh(z1)\n",
    "        z2 = a1.dot(self.W2) + self.b2\n",
    "        y_hat = T.nnet.softmax(z2)\n",
    "        \n",
    "        prediction = T.argmax(y_hat, axis=1)\n",
    "        \n",
    "        self.predict_by_X = theano.function([X], prediction)\n",
    "        \n",
    "    \n",
    "    def predict(self, predict_X):\n",
    "        result = self.predict_by_X(predict_X.astype('float32'))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(random_seed, n_samples):\n",
    "    np.random.seed(random_seed)\n",
    "    X, y = datasets.make_moons(n_samples, noise=0.20)\n",
    "    return X, y\n",
    "\n",
    "def visualize(X, y, model):\n",
    "    plt.title(\"nn_theano_gpu_classification\")\n",
    "    plot_decision_boundary(lambda x:model.predict(x), X, y)\n",
    "\n",
    "def plot_decision_boundary(pred_func, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.475901\n",
      "Loss after iteration 1000: 0.314007\n",
      "Loss after iteration 2000: 0.295341\n",
      "Loss after iteration 3000: 0.284260\n",
      "Loss after iteration 4000: 0.271995\n",
      "Loss after iteration 5000: 0.257798\n",
      "Loss after iteration 6000: 0.242628\n",
      "Loss after iteration 7000: 0.227780\n",
      "Loss after iteration 8000: 0.213996\n",
      "Loss after iteration 9000: 0.201366\n",
      "Loss after iteration 10000: 0.189710\n",
      "Loss after iteration 11000: 0.178847\n",
      "Loss after iteration 12000: 0.168664\n",
      "Loss after iteration 13000: 0.159130\n",
      "Loss after iteration 14000: 0.150283\n",
      "Loss after iteration 15000: 0.142208\n",
      "Loss after iteration 16000: 0.134991\n",
      "Loss after iteration 17000: 0.128687\n",
      "Loss after iteration 18000: 0.123283\n",
      "Loss after iteration 19000: 0.118718\n",
      "training time : 59.8759999275\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    epsilon = 0.01  # learning rate for gradient descent\n",
    "    reg_lambda = 0.01  # regularization strength\n",
    "    layers = [2, 4, 2] # number of nodes in each layer\n",
    "    num_passes = 20000\n",
    "    print_loss = True\n",
    "    random_seed = 6\n",
    "    num_samples = 5000\n",
    "\n",
    "X, y = generate_data(Config.random_seed, Config.num_samples)\n",
    "model = NNModel(Config.layers, Config.epsilon, Config.reg_lambda)\n",
    "\n",
    "# model.train_init(X, y)\n",
    "# %timeit model.gradient_step()\n",
    "start_time = time.time()\n",
    "model.train(X, y, Config.num_passes, Config.print_loss)\n",
    "end_time = time.time()\n",
    "print 'training time : ' + str(end_time - start_time)\n",
    "\n",
    "visualize(X, y, model)\n",
    "#model.gradient_step.profile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
