{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce 820M (CNMeM is enabled with initial size: 95.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import time\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 36.   2.]\n",
      " [ 64.   4.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function profiling\n",
      "==================\n",
      "  Message: None\n",
      "  Time in 1 calls to Function.__call__: 0.000000e+00s\n",
      "  Total compile time: 5.699992e-02s\n",
      "    Number of Apply nodes: 3\n",
      "    Theano Optimizer time: 2.400017e-02s\n",
      "       Theano validate time: 0.000000e+00s\n",
      "    Theano Linker time (includes C, CUDA code generation/compiling): 8.999825e-03s\n",
      "       Import time 0.000000e+00s\n",
      "\n",
      "Time in all call to theano.grad() 0.000000e+00s\n",
      "Time since theano import 711.089s\n",
      "  No execution time accumulated (hint: try config profiling.time_thunks=1)\n",
      "Here are tips to potentially make your code run faster\n",
      "                 (if you think of new ones, suggest them on the mailing list).\n",
      "                 Test them first, as they are not guaranteed to always provide a speedup.\n",
      "  Sorry, no tip for today.\n"
     ]
    }
   ],
   "source": [
    "class Test:\n",
    "    def __init__(self, train_x):\n",
    "        self.x = T.fmatrix('x')\n",
    "        self.y = self.x * 2;\n",
    "        self.pred_f = theano.function([self.x], self.y)\n",
    "    def predict(self, x):\n",
    "        return self.pred_f(np.float32(x))\n",
    "        \n",
    "t = Test(11)\n",
    "print t.pred_f(np.float32([[18, 1],[32, 2]]))\n",
    "\n",
    "x = T.fmatrix('x')\n",
    "y = x * 2\n",
    "profile = theano.compile.ProfileStats()\n",
    "f = theano.function([x],y, profile=profile)\n",
    "f(np.float32([[1,2],[3,4]]))\n",
    "f.profile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NNModel:\n",
    "    def __init__(self, layers, epsilon = 0.01, reg_lambda = 0.01):\n",
    "        self.layers = layers # number of nodes in each layer\n",
    "        self.epsilon = np.float32(epsilon) # learning rate for gradient descent\n",
    "        self.reg_lambda = np.float32(reg_lambda) # regularization strength        \n",
    "        \n",
    "        # Initialize the parameters (W and b) to random values. We need to learn these.        \n",
    "        np.random.seed(int(time.time()) % 1000)\n",
    "        hidden_layer_num = len(layers) - 1\n",
    "        if hidden_layer_num != 2:\n",
    "            print 'only support 2 hidden layer'\n",
    "            exit(0)\n",
    "        self.W1 = theano.shared(np.random.randn(layers[0], layers[1]).astype('float32'))\n",
    "        self.b1 = theano.shared(np.zeros(layers[1]).astype('float32'))\n",
    "        self.W2 = theano.shared(np.random.randn(layers[1], layers[2]).astype('float32'))\n",
    "        self.b2 = theano.shared(np.zeros(layers[2]).astype('float32'))\n",
    "\n",
    "    # This function learns parameters for the neural network from training dataset\n",
    "    # - num_passes: Number of passes through the training data for gradient descent\n",
    "    # - print_loss: If True, print the loss every 1000 iterations\n",
    "    def train(self, train_X, train_y, num_passes=20000, print_loss=False):\n",
    "        num_class = np.max(train_y) + 1\n",
    "        num_examples = len(train_X)\n",
    "        train_y_onehot = np.eye(num_class)[train_y]\n",
    "        \n",
    "        # GPU NOTE: Conversion to float32 to store them on the GPU!\n",
    "        X = theano.shared(train_X.astype('float32'))\n",
    "        y = theano.shared(train_y_onehot.astype('float32'))\n",
    "        \n",
    "        # Forward propagation\n",
    "        z1 = X.dot(self.W1) + self.b1\n",
    "        a1 = T.tanh(z1)\n",
    "        z2 = a1.dot(self.W2) + self.b2\n",
    "        y_hat = T.nnet.softmax(z2)\n",
    "        \n",
    "        #Prediction\n",
    "        prediction = T.argmax(y_hat, axis=1)\n",
    "\n",
    "        #Loss function\n",
    "        loss_reg = 1. / num_examples * self.reg_lambda / 2 * (T.sum(T.sqr(self.W1)) + T.sum(T.sqr(self.W2)))\n",
    "#         loss_reg_ws = 0\n",
    "#         for W in self.Ws:\n",
    "#             loss_reg_ws = loss_reg_ws + T.sum(T.sqr(W))\n",
    "#         loss_reg = 1./num_examples * self.reg_lambda/2 * loss_reg_ws\n",
    "        loss = T.nnet.categorical_crossentropy(y_hat, y).mean() + loss_reg\n",
    "\n",
    "        # Gradients\n",
    "        dW2 = T.grad(loss, self.W2)\n",
    "        db2 = T.grad(loss, self.b2)\n",
    "        dW1 = T.grad(loss, self.W1)\n",
    "        db1 = T.grad(loss, self.b1)\n",
    "\n",
    "        # Note that we removed the input values because we will always use the same shared variable\n",
    "        # GPU NOTE: Removed the input values to avoid copying data to the GPU.\n",
    "        forward_prop = theano.function([], y_hat)\n",
    "        calculate_loss = theano.function([], loss)\n",
    "        predict = theano.function([], prediction)\n",
    "\n",
    "        # GPU NOTE: Removed the input values to avoid copying data to the GPU.\n",
    "        self.gradient_step = theano.function(\n",
    "            [],\n",
    "            # profile=True,\n",
    "            updates=((self.W2, self.W2 - self.epsilon * dW2),\n",
    "                     (self.W1, self.W1 - self.epsilon * dW1),\n",
    "                     (self.b2, self.b2 - self.epsilon * db2),\n",
    "                     (self.b1, self.b1 - self.epsilon * db1)))\n",
    "        \n",
    "        # Gradient descent. For each batch...\n",
    "        for i in xrange(0, num_passes):\n",
    "            # This will update our parameters Ws and bs!\n",
    "            self.gradient_step()\n",
    "\n",
    "            # Optionally print the loss.\n",
    "            # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "            if print_loss and i % 1000 == 0:\n",
    "                print \"Loss after iteration %i: %f\" %(i, calculate_loss())\n",
    "\n",
    "        self.predictInit()\n",
    "\n",
    "    def predictInit(self):\n",
    "        X = T.fmatrix('X')\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = X.dot(self.W1) + self.b1\n",
    "        a1 = T.tanh(z1)\n",
    "        z2 = a1.dot(self.W2) + self.b2\n",
    "        y_hat = T.nnet.softmax(z2)\n",
    "        \n",
    "        prediction = T.argmax(y_hat, axis=1)\n",
    "        \n",
    "        self.predict_by_X = theano.function([X], prediction)\n",
    "    \n",
    "    def predictExecute(self, predict_X):\n",
    "        return self.predict_by_X(predict_X.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(random_seed, n_samples):\n",
    "    np.random.seed(random_seed)\n",
    "    X, y = datasets.make_moons(n_samples, noise=0.20)\n",
    "    return X, y\n",
    "\n",
    "def visualize(X, y, model):\n",
    "    plt.title(\"tanh_cross_entropy_ann_classification\")\n",
    "    plot_decision_boundary(lambda x:model.predictExecute(x), X, y)\n",
    "\n",
    "def plot_decision_boundary(pred_func, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.436574\n",
      "Loss after iteration 1000: 0.291402\n",
      "Loss after iteration 2000: 0.285377\n",
      "Loss after iteration 3000: 0.281749\n",
      "Loss after iteration 4000: 0.278547\n",
      "Loss after iteration 5000: 0.275202\n",
      "Loss after iteration 6000: 0.271164\n",
      "Loss after iteration 7000: 0.265419\n",
      "Loss after iteration 8000: 0.255923\n",
      "Loss after iteration 9000: 0.240057\n",
      "Loss after iteration 10000: 0.219436\n",
      "Loss after iteration 11000: 0.200856\n",
      "Loss after iteration 12000: 0.186399\n",
      "Loss after iteration 13000: 0.174619\n",
      "Loss after iteration 14000: 0.164550\n",
      "Loss after iteration 15000: 0.155717\n",
      "Loss after iteration 16000: 0.147826\n",
      "Loss after iteration 17000: 0.140683\n",
      "Loss after iteration 18000: 0.134160\n",
      "Loss after iteration 19000: 0.128179\n",
      "1000 loops, best of 3: 1.44 ms per loop\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    epsilon = 0.01  # learning rate for gradient descent\n",
    "    reg_lambda = 0.01  # regularization strength\n",
    "    layers = [2, 4, 2] # number of nodes in each layer\n",
    "    num_passes = 20000\n",
    "    print_loss = True\n",
    "    \n",
    "    random_seed = 6\n",
    "    num_samples = 2000\n",
    "\n",
    "X, y = generate_data(Config.random_seed, Config.num_samples)\n",
    "model = NNModel(Config.layers, Config.epsilon, Config.reg_lambda)\n",
    "model.train(X, y, Config.num_passes, Config.print_loss)\n",
    "visualize(X, y, model)\n",
    "%timeit model.gradient_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
